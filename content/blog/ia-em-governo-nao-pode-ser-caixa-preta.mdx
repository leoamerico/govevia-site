---
title: "Por que IA em governo não pode ser ‘caixa preta’"
date: "2026-02-16"
description: "Como usar IA de forma responsável no setor público: escopo assistivo, humano-no-circuito, validação determinística e trilha de evidência para auditoria."
author: "Env Neo Ltda."
draft: true
tags:
  - GovTech
  - IA aplicada
  - Governança Digital
  - Auditoria
  - Controle Interno
---

## O problema: automação sem responsabilidade vira risco institucional

No setor público, a pergunta central não é apenas “o modelo acertou?”. É: **quem responde pelo ato**, **com base em qual norma** e **qual evidência sustenta a decisão**.

Quando uma recomendação de IA vira decisão sem critérios claros, a organização cria vulnerabilidade:

- Dificuldade de reconstituir o contexto (dados, norma, premissa e responsável)
- Fragilidade em auditoria e controle interno
- Risco de padronizar erro operacional em escala

## IA assistiva, não decisória

Em sistemas de gestão pública, a IA pode ser valiosa como suporte: leitura, triagem, sugestão de enquadramento e redação técnica. Mas isso não deve substituir as camadas de governança.

Uma postura segura começa por um princípio simples:

Nenhuma decisão administrativa deve ser tomada exclusivamente por IA.

Isso significa que recomendações precisam de:

- Revisão humana registrada (identificação do responsável)
- Base normativa consultada ou utilizada como referência
- Critérios determinísticos de validação (regras e checagens técnicas)

## “Falha segura” aplicada a IA

IA não elimina incerteza. O que muda é como o sistema reage quando há dúvida.

Uma postura de falha segura, em termos práticos, significa:

- Se faltar requisito essencial, o fluxo pode exigir validação antes de prosseguir
- Se a norma exigir documento/parecer/assinatura, o sistema deve impedir consolidação sem a etapa
- Se houver conflito entre recomendação e regra determinística, prevalece a regra

## O que é auditável em IA no setor público

Para ser auditável, o sistema precisa produzir evidência operacional suficiente para reconstrução do ato. Em cenários com IA, isso inclui (quando aplicável):

- Versão do modelo e do prompt/política utilizada
- Entrada considerada relevante (com proteção de dados e minimização)
- Saída gerada e o que foi aceito/alterado pelo responsável
- Regra/checagem determinística aplicada e resultado

## Como a Govevia posiciona IA (claim governável)

Na Govevia, modelos de IA podem sugerir enquadramentos normativos e apoiar a análise, mas a consolidação de atos depende de validação determinística e de revisão humana registrada.

Nenhuma decisão administrativa é tomada exclusivamente por IA.

Em caso de dúvida, o fluxo pode exigir validação antes de prosseguir.

---

<ViewBlock view="prefeito">

## Visão: Prefeito

IA no setor público é um recurso de apoio à gestão, não uma substituição ao ato administrativo. O que importa para o gestor máximo: **quem assina**, **qual norma ampara** e **o que serve de evidência** em caso de questionamento.

Um sistema que recomenda sem registrar — ou que consolida sem revisão humana — transfere risco ao gestor sem oferecer proteção. O modelo correto mantém o humano no circuito e a evidência no registro.

### Limites e Condições

- A responsabilidade pelo ato administrativo permanece com o agente público designado.
- IA pode sugerir enquadramento normativo; a decisão de aplicar é do gestor.

### Evidências

- Ver: docs/public/evidence/WEB-CLAIMS-REPORT.md

</ViewBlock>

<ViewBlock view="procurador">

## Visão: Procurador

Do ponto de vista jurídico, a questão central é: **o ato é reconstituível?** Recomendações de IA que influenciam decisões administrativas precisam deixar rastro suficiente para que qualquer revisor posterior consiga reconstituir: qual dado entrou, qual norma foi consultada, o que o sistema sugeriu e o que o responsável aceitou ou alterou.

Sem essa cadeia, a defesa do ato fica fragilizada e a responsabilidade difusa — exatamente o cenário que sistemas de gestão pública devem evitar.

### Limites e Condições

- Este texto não substitui parecer jurídico.
- Os requisitos de rastreabilidade variam por tipo de ato e órgão controlador.

### Evidências

- Ver: docs/public/evidence/REPORT-CLAIMS-IMPLEMENTATION.md

</ViewBlock>

<ViewBlock view="controlador">

## Visão: Controlador

Para auditoria, o que não foi registrado não aconteceu. Sistemas com IA precisam produzir **evidência operacional**: versão do modelo e da política, entrada relevante com minimização de dados, saída gerada e o que foi aceito/alterado, e a regra determinística que prevaleceu em caso de conflito.

Qualquer componente "caixa preta" — que gera output sem rastro — é incompatível com auditoria efetiva.

### Limites e Condições

- Evidência só serve se for versionada, imutável e reprodutível.
- Mecanismos de hash e cadeia de custódia são requisito, não opcional.

### Evidências

- Ver: docs/public/evidence/REPORT-CLAIMS-IMPLEMENTATION.md

</ViewBlock>

<ViewBlock view="secretario">

## Visão: Secretário

Na operação diária, o risco de IA "caixa preta" se manifesta como **padronização de erro em escala**: sem visibilidade sobre o que o modelo fez, é impossível detectar onde a cadência de verificação falhou ou qual gate não foi aplicado.

A postura correta é operacionalizar controles verificáveis: checklist de revisão humana, runbooks para casos de dúvida, e métricas que mostrem quando o sistema está fora dos parâmetros esperados.

### Limites e Condições

- Requer governança operacional com rotinas, métricas e revisão periódica.
- Conformidade não é um estado — é uma cadência de verificação contínua.

### Evidências

- Ver: docs/public/evidence/RUNBOOK-CONTACT-API-CSRF.md

</ViewBlock>

---

*Este texto tem caráter técnico-informativo e não substitui parecer jurídico ou orientação específica de órgãos de controle.*
